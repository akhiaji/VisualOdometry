{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Match depth images to the RGB images\n",
    "- Read the first image from file and it's ground truth pose as reference.\n",
    "- After the first image, read one image at a time and calculate it's tranformation from the previous image.\n",
    "    - To calculate the tranformation, first extract SIFT descriptors and for both images and find matches. Then use the SVD based formula to calculate the transformation from the images and apply the tranformation to the pose of previous time instant to calculate the new pose. Convert the rotation matrix into unit quaternions.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('matched.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "lines = data.replace(\",\",\" \").replace(\"\\t\",\" \").split(\"\\n\") \n",
    "matches = [[v.strip() for v in line.split(\" \") if v.strip()!=\"\"] for line in lines if len(line)>0 and line[0]!=\"#\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matches(img1, img2, src_pts, dst_pts, kp1, kp2, good):\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "    h,w = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "\n",
    "    img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\n",
    "    \n",
    "    plt.imshow(img3, 'gray'),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matches(img1, img2):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    MIN_MATCH_COUNT = 10\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "            \n",
    "    if len(good)>MIN_MATCH_COUNT:\n",
    "        src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "#         visualize_matches(img1, img2, src_pts, dst_pts, kp1, kp2, good)\n",
    "        return src_pts, dst_pts\n",
    "    else:\n",
    "        print(\"Not enough matches are found\")\n",
    "        matchesMask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_points(pts_1, pts_2, depths_1, depths_2):\n",
    "    depths_1 = np.array([depths_1]).T\n",
    "    depths_2 = np.array([depths_2]).T\n",
    "    \n",
    "    pts_1 = np.append(pts_1, depths_1, axis=1)\n",
    "    pts_2 = np.append(pts_2, depths_2, axis=1)\n",
    "    all_points = np.append(pts_1, pts_2, axis=1)\n",
    "    all_points = all_points[np.where(np.logical_and(all_points[:,2] > 0, all_points[:,5] > 0))]\n",
    "\n",
    "    pts_1 = all_points[:,:3]\n",
    "    pts_1[:,0] = (pts_1[:,0] - 239.5)/525.0\n",
    "    pts_1[:,0] = pts_1[:,0]*pts_1[:,2]\n",
    "    pts_1[:,1] = (pts_1[:,1] - 319.5)/525.0\n",
    "    pts_1[:,1] = pts_1[:,1]*pts_1[:,2]\n",
    "\n",
    "    pts_2 = all_points[:,3:]\n",
    "    pts_2[:,0] = (pts_2[:,0] - 239.5)/525.0\n",
    "    pts_2[:,0] = pts_2[:,0]*pts_2[:,2]\n",
    "    pts_2[:,1] = (pts_2[:,1] - 319.5)/525.0\n",
    "    pts_2[:,1] = pts_2[:,1]*pts_2[:,2]\n",
    "    return pts_1, pts_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transformation(pts_1, pts_2):\n",
    "    \"\"\"\n",
    "    Given two sets of points calculate the rotation and translation of the camera pose\n",
    "    \"\"\"\n",
    "    mean_1 = np.mean(pts_1, axis=0)\n",
    "    mean_2 = np.mean(pts_2, axis=0)\n",
    "\n",
    "    pts_1 = pts_1 - mean_1\n",
    "    pts_2 = pts_2 - mean_2\n",
    "\n",
    "    mat = np.dot(pts_1.T, pts_2)\n",
    "    u, s, v = np.linalg.svd(mat)\n",
    "    R = np.dot(v, u.T)\n",
    "    t = mean_2 - np.dot(R,mean_1)\n",
    "    \n",
    "    return R, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quaternions(R):\n",
    "    tr = R[0][0] + R[1][1] + R[2][2]\n",
    "\n",
    "    if (tr > 0):\n",
    "        S = np.sqrt(tr+1.0) * 2\n",
    "        qw = 0.25 * S;\n",
    "        qx = (R[2][1] - R[1][2]) / S\n",
    "        qy = (R[0][2] - R[2][0]) / S \n",
    "        qz = (R[1][0] - R[0][1]) / S\n",
    "    elif((R[0][0] > R[1][1]) and (R[0][0] > R[2][2])):\n",
    "        S = np.sqrt(1.0 + R[0][0] - R[1][1] - R[2][2]) * 2\n",
    "        qw = (R[2][1] - R[1][2]) / S\n",
    "        qx = 0.25 * S\n",
    "        qy = (R[0][1] + R[1][0]) / S \n",
    "        qz = (R[0][2] + R[2][0]) / S\n",
    "    elif (R[1][1] > R[2][2]): \n",
    "        S = np.sqrt(1.0 + R[1][1] - R[0][0] - R[2][2]) * 2\n",
    "        qw = (R[0][2] - R[2][0]) / S\n",
    "        qx = (R[0][1] + R[1][0]) / S \n",
    "        qy = 0.25 * S\n",
    "        qz = (R[1][2] + R[2][1]) / S\n",
    "    else:\n",
    "        S = np.sqrt(1.0 + R[2][2] - R[0][0] - R[1][1]) * 2\n",
    "        qw = (R[1][0] - R[0][1]) / S\n",
    "        qx = (R[0][2] + R[2][0]) / S\n",
    "        qy = (R[1][2] + R[2][1]) / S\n",
    "        qz = 0.25 * S;\n",
    "    return qx, qy, qz, qw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_matrix(pose):\n",
    "    pose_matrix = np.zeros((4,4))\n",
    "    t = pose[0]\n",
    "    q = pose[1]\n",
    "    x = q[0]\n",
    "    y = q[1]\n",
    "    z = q[2]\n",
    "    w = q[3]\n",
    "    R = np.zeros((3,3))\n",
    "    R[0][0] = 1 - 2*(y**2 + z**2)\n",
    "    R[0][1] = 2*(x*y - z*w)\n",
    "    R[0][2] = 2*(x*z + y*w)\n",
    "    R[1][0] = 2*(x*y + z*w)\n",
    "    R[1][1] = 1 - 2*(x**2 + z**2)\n",
    "    R[1][2] = 2*(y*z - x*w)\n",
    "    R[2][0] = 2*(x*z - y*w)\n",
    "    R[2][1] = 2*(y*z + x*w)\n",
    "    R[2][2] = 1 - 2*(x**2 + y**2)\n",
    "    \n",
    "    pose_matrix[:3,:3] = R\n",
    "    pose_matrix[:3, 3] = t\n",
    "    pose_matrix[3,3] = 1\n",
    "    return pose_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305031102.226738 [array([-0.14356612, -1.07928301,  0.4403204 ]), array([-0.1931933 ,  0.9763566 ,  0.09655318,  0.00123194])]\n",
      "1305031102.262886 [array([ 0.18049132, -0.99793439,  0.31389873]), array([ 0.78456352, -0.42887962, -0.31108499, -0.32196764])]\n",
      "1305031102.295279 [array([ 0.24839762, -0.91368663,  0.30715792]), array([ 0.74832467,  0.44045024, -0.29226428, -0.40064757])]\n",
      "1305031102.329195 [array([ 0.26527623, -0.9290569 ,  0.30779214]), array([ 0.14536328,  0.97137422, -0.05023226, -0.180868  ])]\n",
      "1305031102.363013 [array([ 0.16940986, -0.99605179,  0.37776175]), array([-0.5957236 ,  0.73494825,  0.25364856,  0.20139412])]\n",
      "1305031102.394772 [array([-0.04102452, -1.13731338,  0.49987088]), array([ 0.81152471,  0.24194956, -0.35691489, -0.39425175])]\n",
      "1305031102.427815 [array([ 0.0624689 , -0.95193271,  0.36992353]), array([ 0.23303094,  0.94218133, -0.11195425, -0.21307703])]\n",
      "1305031102.462395 [array([-0.056387  , -1.16320625,  0.44831283]), array([-0.5930741 ,  0.73811831,  0.2776017 ,  0.16225446])]\n",
      "1305031102.526330 [array([ 0.30139005, -0.8207826 ,  0.21395439]), array([ 0.83564284,  0.02981845, -0.36100657, -0.41283445])]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "pose_1 = [np.array([1.3434, 0.6271, 1.6606]), np.array([0.6583, 0.6112, -0.2938, -0.3266])]\n",
    "rgbimage_1 = cv2.imread(matches[index][3], 0)\n",
    "depthimage_1 = cv2.imread(matches[index][1], -1)\n",
    "index += 1\n",
    "while(index < len(matchess)):\n",
    "    new_rgbimage = cv2.imread(matches[index][3], 0)\n",
    "    new_depthimage = cv2.imread(matches[index][1], -1)\n",
    "    src_pts, dst_pts = compute_matches(new_rgbimage, rgbimage_1)\n",
    "    \n",
    "    pts_1 = np.squeeze(src_pts)\n",
    "    pts_2 = np.squeeze(dst_pts)\n",
    "    \n",
    "    depths_1 = depthimage_1[np.int32(pts_1[:, 1]), np.int32(pts_1[:, 0])]/5000\n",
    "    depths_2 = new_depthimage[np.int32(pts_2[:, 1]), np.int32(pts_2[:, 0])]/5000\n",
    "    \n",
    "    pts_1, pts_2 = transform_points(pts_1, pts_2, depths_1, depths_2)\n",
    "    \n",
    "    R, t = calculate_transformation(pts_1, pts_2)\n",
    "    new_posematrix = np.zeros((4,4))\n",
    "    new_posematrix[:3,:3] = R\n",
    "    new_posematrix[:3,3] = t\n",
    "    new_posematrix[3,3] = 1\n",
    "    \n",
    "    posematrix_1 = get_pose_matrix(pose_1)\n",
    "    result_pose = np.dot(posematrix_1, new_posematrix)\n",
    "    \n",
    "    qx, qy, qz, qw = get_quaternions(result_pose[:3,:3])\n",
    "    pose_1 = [new_posematrix[:3,3], np.array([qx, qy, qz, qw])]\n",
    "#     qx, qy, qz, qw = get_quaternions(R)\n",
    "#     v1 = np.array([qx, qy, qz])\n",
    "#     w1 = qw\n",
    "#     v0 = pose_1[1][:3]\n",
    "#     w0 = pose_1[1][3]\n",
    "# #     q 2 = q 0 q 1 = (v 0 × v 1 + w 0 v 1 + w 1 v 0 , w 0 w 1 − v 0 · v 1 )\n",
    "#     new_v = np.cross(v0,v1) + w0*v1 + w1*v0\n",
    "#     new_w = w0*w1 - np.dot(v0,v1)\n",
    "    \n",
    "#     pose_1 = [t, np.append(new_v, new_w)]\n",
    "    rgbimage_1 = new_rgbimage\n",
    "    depthimage_1 = new_depthimage\n",
    "    print(matches[index][0], pose_1)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1305031102.160407 1.344379 0.627206 1.661754 0.658249 0.611043 -0.294444 -0.326553\n",
    "#1305031102.194330 1.343641 0.626458 1.652408 0.657327 0.613265 -0.295150 -0.323593\n",
    "#1305031102.226738 1.338382 0.625665 1.641460 0.657713 0.615255 -0.294626 -0.319485\n",
    "#1305031102.262886 1.325627 0.624485 1.632561 0.659141 0.617445 -0.292536 -0.314195"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
