<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | CS, Georgia Tech | Fall 2019: CS 6476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}

table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td, th {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name -->
<h1>Visual Odometry</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Team VOD: Akhilesh Aji, Bhavya Bahl, Ashwin Jeyaseelan</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2019 CS 6476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal -->
<h3>Abstract</h3>

We will develop a visual odometry system that tracks the pose of the camera throughout a video sequence. This involves elements of feature detection and matching, image rotation and translation, and consensus gathering to build a complex system.<br><br>
<!-- figure -->
<h4>Visual Odometry in Practice:</h4>
<br><br>
<!-- Main Illustrative Figure -->
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="vo.jpg">
</div>
<br>
This approach is demonstrated above in the driverless car domain. The image is showing feature matching and a path being created out of the images that are collected.
<br><br>
<!-- Introduction -->
<h3>Introduction</h3>
One of the most fundamental algorithms for mobile robotics is Simultaneous Localization And Mapping (SLAM). SLAM is used by a robot to discern where in the world it is, and build a map around itself. In this project we plan on replicating a crucial component of SLAM called Visual Odometry(VO). VO is the process of using a camera to verify the position of a robot. This can be very useful as the redundancy in odometry helps to reduce drift in the system as error slowly builds up.
<br><br>
The system we will be developing will take in an RGB-D video feed generated by a kinect and will plot the movement of the camera around the world. Our system will tag each frame of the video with the (x,y,z, unit quaternion) of the camera when it took this image based on the predicted position of the previous image and transformation between the two images.
<br><br>
This problem is particularly interesting because it combines many of the tools we have discussed in class in a layered fashion. Edges must be extracted to help define features for SIFT to extract and match followed by finding the homography matrix to understand the relationship between each of the images. Each level of the process will require that the previous step was done with as little noise seeping through as possible. This means that there will always be additional features we can implement to reduce noise or allow the system to run faster. In addition visual odometry as a subject is still under research. As robots are becoming more ubiquitous and cheap they use fewer and cheaper sensors. As a result, simple cameras are doing the job that previously was accomplished with other sensors such as lidar. While our primary goal is to get visual odometry working with color and depth information we will compare its performance with that of a monocular system which is currently on the cutting edge of research.
<br><br>


<br><br>
<!-- Approach -->
<h3>Approach</h3>

  In visual odometry, we are given a camera which is moving through space and capturing images at discrete time steps. The goal is to predict the pose of the camera at each time step with respect to it's initial pose. Let us denote the set of images taken at times k by \(I_{0: n}=\left\{I_{0}, \ldots, I_{n}\right\}\). In the dataset we plan to use, the images contain a depth component along with the RGB components and the pixels in the frames of depth videos correspond 1:1 to the pixels in the frames of RGB videos.
  <br><br>
  The movement of camera from one position to another can be modelled as a rigid body transformation given by \(T_{k, k-1} \in \mathbb{R}^{4 \times 4}\):

  \[T_{k, k-1}=\left[\begin{array}{cc}{R_{k, k-1}} & {t_{k, k-1}} \\ {0} & {1}\end{array}\right]\]

  where \(R_{k,k-1}\) is a \(3 \times 3\) rotation matrix and \(t_{k,k-1} \in \mathbb{R}^{3 \times 1}\) is the translation vector. An important component of visual odometry is to compute the relative transformation \(T_{k,k-1}\) from the images \(I_{k}\) and \(I_{k-1}\). To compute the camera pose at any time instant, we apply the transformations computed at each time step to initial pose. Let the set of camera poses be \(C_{0:n} = \left\{C_0, \ldots, C_n\right\}\), then \(C_n = C_{n-1}T_{n,n-1}\), with \(C_0\) being the initial camera pose, which can be set arbitrarily.
  <br><br>
  To compute transformations between images, we will first extract salient features from RGB images, which are likely to match across consecutive time instants. We will concatenate the depth information to the (x,y) location of each of the keypoints detected to get 3D features as described <a href="https://vision.in.tum.de/data/datasets/rgbd-dataset/file_formats#intrinsic_camera_calibration_of_the_kinect">here</a>. This webpage explains how to go from 2D points to 3D point clouds. The most relevant features for visual odometry are corners and blobs because their positions in can be measured accurately. There is vast literature on both corner detectors (like Moravec, Forstner, Harris, and FAST) and blob detectors(SIFT, SURF, CENSURE). We will explore some of these methods to perform feature detection. After detecting features, we will match features between images at consecutive time instants using the similarity of feature descriptors like SIFT. After computing the 3D-to-3D correspondences between the features of the images, we want to find \(T_k\) that minimizes the \(L_2\) distance between the two 3-D feature sets. The rotation and translation can be computed as:
  \[R_{k,k-1} = VU^{\top}, \quad t_{k,k-1}=\overline{X_{k}}-R_{k,k-1} \overline{X}_{k-1} \]
  where \(X_k\) and \(X_{k-1}\) are the set of corresponding 3-D points and \(\mathrm{USV}^{\top}=\operatorname{svd}\left(\left(X_{k-1}-\overline{X}_{k-1}\right)\left(X_{k}-\overline{X}_{k}\right)^{\top}\right)\) and \(\overline{\cdot}\) stands for arithmetic mean value. This approach is described in [2]. Once we have the transformation matrices, we can get the camera pose at each time instant simply by concatenating the transformations.


<br><br>
<!-- Results -->
<h3>Experiments and results</h3>

We tested our algorithm on the <a href="https://vision.in.tum.de/data/datasets/rgbd-dataset/download#freiburg1_floor">freiburg1_floor</a> dataset from the computer vision group of the Technical University of Munich Department of Informatics [1]. The data includes a video going over the wooden floor of an office, with rgb-data and ground truth camera pose information. 
<br><br>

<!-- Main Results Figure -->
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="data.png">
</div>
<br>
We have implemented two different versions of the algorithm. The first is based on the epipolar geometry discussed in class. This method does not take advantage of the depth component of the dataset. While implementng this we were not able to get consistent results. We have decided not to continue pursuing this path as it does not take full advantage of the data we are given, and was the worse performing of the two routes we pursued. The second method which is the one we initially proposed in our approach is based on the work from Arun et al.[2].
<br><br>
The code that we wrote can be found <a href="https://github.com/akhiaji/VisualOdometry"> here</a>. The SIFT based feature matching code is based on [4].
<br><br>
The image below compares our recreated odometry with the ground truth x, y coordinates of the camera. In both ground truth and our recreation, the path starts by going to the left then moving in the y dimension before jumping back to the right and finishing the path on that side. The results aren’t perfect, but our algorithm does a good job predicting the overall path. It’s also important to consider the fact that even the ground truth isn’t a continuous plot, due to the video jumping frames in the middle of the sequence. 
<br><br>

<div style="text-align: center;">
<img style="height: 600px;" alt="" src="comparison.png">
</div>
<br><br>
Below is a plot of the difference between our estimated path and the ground truth 2D camera pose coordinates. 

<div style="text-align: center;">
  <img style="height: 400px;" alt="" src="path_diff.png">
</div>

<br><br>

<br><br>
As shown in the plots above and below, the difference increases over time. For roughly the first half of the trajectory, the difference is small until it jumps to the right and the error accumulates. 
<br><br>

<div style="text-align: center;">
  <img style="height: 200px;" alt="" src="error_time.png">
</div>

<br><br>

The error per step helps explain this. Fortunately, during most of the time steps, the error is close to 0. 
But at certain steps, there are spikes in the error, such as around 750 and 1000. A possible cause for this 
issue is that the video skipped a bunch of frames at these time steps, resulting in an inaccurate transformation 
matrix across the subsequent frames at those time steps and consequently incorrect pose predictions. This error offsets every pose for the rest of the path. For example, if only the first 750 estimated poses are used the Absolute translational error RMSE is 0.0857 meters in addition the 250 poses between 750 and 1000 the error is 0.12. As you can see this means that before and after the spike in error the tracking is very good. As a result, in a real world situation in which every frame is being processed the error would remain very low.
<br><br>

Below is a table of our metrics (in meters) to further evaluate our algorithm. 
The root mean squared error is sensitive to outliers, which our data clearly has, 
as shown by the standard deviation in error and the difference between the minimum 
and maximum error values. Therefore, the mean and median are helpful to look at, 
which are around 0.4 meters.
<br><br>
<table style="width:50%">
  <tr>
    <th>Metrics for ratio test value of 0.4</th>
    <th>Value</th>
  </tr>
  <tr>
    <td>Absolute translational error RMSE</td>
    <td>0.613676 m</td>
  </tr>
  <tr>
    <td>Absolute translational error mean</td>
    <td>0.498433 m</td>
  </tr>
  <tr>
    <td>Absolute translational error median</td>
    <td>0.415210 m</td>
  </tr>
  <tr>
    <td>Absolute translational error std</td>
    <td>0.357998 m</td>
  </tr>
  <tr>
    <td>Absolute translational error minimum</td>
    <td>0.026930 m</td>
  </tr>
  <tr>
    <td>Absolute translational error max</td>
    <td>2.755501 m</td>
  </tr>
</table>
<br><br>

One parameter that we experimented with was the ratio value utilized in the ratio test, when selecting matching feature points between frames. 
The idea behind the ratio test is to filter out bad matches based on a threshold ratio between the best and second best match distance [5]. 
Decreasing the ratio value results in a higher threshold, which helps filter out more ambiguous points because in order for a match to be reliable, 
the closest point in a correct match should be much closer than any other incorrect point [5]. The above list has better results with a ratio value of 0.4. 
With an increased ratio value of 0.5, the errors increase as shown below:

<br><br>

<table style="width:50%">
  <tr>
    <th>Metrics for ratio test value of 0.5</th>
    <th>Value</th>
  </tr>
  <tr>
    <td>Absolute translational error RMSE</td>
    <td>1.781425 m</td>
  </tr>
  <tr>
    <td>Absolute translational error mean</td>
    <td>1.461322 m</td>
  </tr>
  <tr>
    <td>Absolute translational error median</td>
    <td>1.088532 m</td>
  </tr>
  <tr>
    <td>Absolute translational error std</td>
    <td>1.018830 m</td>
  </tr>
  <tr>
    <td>Absolute translational error minimum</td>
    <td>0.43635 m</td>
  </tr>
  <tr>
    <td>Absolute translational error max</td>
    <td>3.686539 m</td>
  </tr>
</table>
<br><br>

<h3>Conclusion and future work</h3>
In conclusion a fairly accurate tracker can be developed using RGB-D data. While the system still has some issues to work out for hundreds of frames at a time the system was able to accurately track the movement around an indoor environment. The main issue to tackle with the current model is getting rid of the spikes. This requires significantly more robustness. There are several papers that discuss how to achieve this robustness that we were not able to implement due to time constraints. One of the prime candidates to boost the accuracy would be to implement the salient point detection described by Li et al.[6]

<h3> References </h3>
[1]Sturm, Jürgen, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. "A benchmark for the evaluation of RGB-D SLAM systems." In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 573-580. IEEE, 2012.
<br>
[2] K. S. Arun, T. S. Huang, and S. D. Blostein, “Least-squares fitting of two 3-d point sets,”IEEE Trans. Pattern Anal. Machine Intell.,vol.9,no.5,pp. 698–700, 1987.
<br>
[3] Steinbrücker, Frank, Jürgen Sturm, and Daniel Cremers. "Real-time visual odometry from dense RGB-D images." In 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), pp. 719-722. IEEE, 2011.
<br>
[4] <a href="https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html">https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html</a>
<br>
[5] Lowe, G. David, "Distinctive Image Features from Scale-Invariant Keypoints." In 2004 International Journal of Computer Vision.
<br>
[6] Li, Shile, and Dongheui Lee. "Fast visual odometry using intensity-assisted iterative closest point." IEEE Robotics and Automation Letters 1.2 (2016): 992-999.


  <hr>
  <footer>
  <p>© Akhilesh Aji, Ashwin Jeyaseelan, Bhavya Bahl</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
