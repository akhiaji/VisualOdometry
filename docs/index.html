<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | CS, Virginia Tech | Fall 2019: CS 6476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name -->
<h1>Visual Odometry</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Team VOD: Akhilesh Aji, Bhavya Bahl, Ashwin Jeyaseelan</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2019 CS 6476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal -->
<h3>Abstract</h3>

We will develop a visual odometry system that tracks the pose of the camera throughout a video sequence. This involves elements of feature detection and matching, image rotation and translation, and consensus gathering to build a complex system.<br><br>
<!-- figure -->
<h3>Teaser figure</h3>
A figure that conveys the main idea behind the project or the main application being addressed.
<br><br>
<!-- Main Illustrative Figure -->
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="mainfig.png">
</div>

<br><br>
<!-- Introduction -->
<h3>Introduction</h3>
One of the most fundamental algorithms for mobile robotics is Simultaneous Localization And Mapping (SLAM). SLAM is used by a robot to discern where in the world it is, and build a map around itself. In this project we plan on replicating a crucial component of SLAM called Visual Odometry(VO). VO is the process of using a camera to verify the position of a robot. This can be very useful as the redundancy in odometry helps to reduce drift in the system as error slowly builds up.
<br><br>
The system we will be developing will take in an RGB-D video feed generated by a kinect and will plot the movement of the camera around the world. Our system will tag each frame of the video with the (x,y,z, unit quaternion) of the camera when it took this image based on the predicted position of the previous image and transformation between the two images.
<br><br>
This problem is particularly interesting because it combines many of the tools we have discussed in class in a layered fashion. Edges must be extracted to help define features for SIFT to extract and match followed by finding the homography matrix to understand the relationship between each of the images. Each level of the process will require that the previous step was done with as little noise seeping through as possible. This means that there will always be additional features we can implement to reduce noise or allow the system to run faster. In addition visual odometry as a subject is still under research. As robots are becoming more ubiquitous and cheap they use fewer and cheaper sensors. As a result, simple cameras are doing the job that previously was accomplished with other sensors such as lidar. While our primary goal is to get visual odometry working with color and depth information we will compare its performance with that of a monocular system which is currently on the cutting edge of research.

<br><br>
<!-- Approach -->
<h3>Approach</h3>

  In visual odometry, we are given a camera which is moving through space and capturing images at discrete time steps and we want to predict the pose of the camera at each time step with respect to it's initial pose. Let us denote the set of images taken at times k by \(I_{0: n}=\left\{I_{0}, \ldots, I_{n}\right\}\). In our case, the images contain a depth component along with the RGB components and in the dataset we plan to use, the pixels in the frames of depth videos correspond 1:1 to the pixels in the frames of RGB videos. The movement of camera from one position to another can be modelled as a rigid body transformation given by \(T_{k, k-1} \in \mathbb{R}^{4 \times 4}\):
  
  \[T_{k, k-1}=\left[\begin{array}{cc}{R_{k, k-1}} & {t_{k, k-1}} \\ {0} & {1}\end{array}\right]\]
  
  where \(R_{k,k-1}\) is a \(3 \times 3\) rotation matrix and \(t_{k,k-1} \in \mathbb{R}^{3 \times 1}\) is the translation vector. An important component of visual odometry is to compute the relative transformations \(T_{k,k-1}\) from the images \(I_{k}\) and \(I_{k-1}\). After computing the transformations, we apply the transformation at each time step to the pose at previous time step to compute the pose at current time. Let the set of camera poses be \(C_{0:n} = \left\{C_0, \ldots, C_n\right\}\), then \(C_n = C_{n-1}T_{n,n-1}\), with \(C_0\) being the initial camera pose which can be set arbitrarily.
  <br><br>
  To compute transformations between images, we will first extract salient features which are likely to match across different RGB images. The most relevant features for visual odometry are corners and blobs because their positions in can be measured accurately. Their is vast literature on both corner detectors (like Moravec, Forstner, Harris, and FAST) and blob detectors(SIFT, SURF, CENSURE). We will explore some of these methods to detect features in images. After detecting features, we will match features between images at consecutive time instants using the similarity of feature descriptors like SIFT. If time permits, we will implement RANSAC to handle outliers in a better way for feature matching and computing transformations. We can get depth for the matched features using the corresponding depth images. Given the 3D-to-3D correspondences between images, we want to find \(T_k\) that minimizes the \(L_2\) distance between two 3-D feature sets. The rotation and translation can be computed as: 
  \[R_{k,k-1} = VU^{\top}, \quad t_{k,k-1}=\overline{X_{k}}-R_{k,k-1} \overline{X}_{k-1} \]
  where \(X_k\) and \(X_{k-1}\) are the set of corresponding 3-D points and \(\mathrm{USV}^{\top}=\operatorname{svd}\left(\left(X_{k-1}-\overline{X}_{k-1}\right)\left(X_{k}-\overline{X}_{k}\right)^{\top}\right)\) and \(\overline{\cdot}\) stands for arithmetic value.
  

<br><br>
<!-- Results -->
<h3>Experiments and results</h3>
Provide details about the experimental set up (number of images/videos, number of datasets you experimented with, train/test split if you used machine learning algorithms, etc.). Describe the evaluation metrics you used to evaluate how well your approach is working. Include clear figures and tables, as well as illustrative qualitative examples if appropriate. Be sure to include obvious baselines to see if your approach is doing better than a naive approach (e.g. for classification accuracy, how well would a classifier do that made random decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance varies as you change those parameter values. Be sure to discuss any trends you see in your results, and explain why these trends make sense. Are the results as expected? Why?

<br><br>

<!-- Main Results Figure -->
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="results.png">
</div>
<br><br>

<!-- Results -->
<h3>Qualitative results</h3>
Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach.
<br><br>

<!-- Main Results Figure -->
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="qual_results.png">
</div>
<br><br>




  <hr>
  <footer>
  <p>Â© You Name Here</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
